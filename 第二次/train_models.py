# train_models.py
#
# åŠŸèƒ½ç¸½è¦½ï¼š
# 1. å¾ train_split.create_dataloaders() å–å¾— train / val / test DataLoader
# 2. æä¾›ä¸‰ç¨®æ¨¡å‹å¯é¸ï¼š
#    - "cnn"       -> SimpleCNN4Blockï¼ˆ4 å€‹å·ç© Blockï¼‰
#    - "resnet18"  -> torchvision.models.resnet18ï¼ˆé è¨“ç·´ï¼‰
#    - "resnet34"  -> torchvision.models.resnet34ï¼ˆé è¨“ç·´ï¼‰
# 3. ä½¿ç”¨ Adam + lr=1e-3 è¨“ç·´
# 4. æ¯å€‹ epoch è¨˜éŒ„ train / val loss & accï¼Œæœ€å¾Œç•«åœ–å­˜æª”
# 5. ç”¨ã€Œé©—è­‰æå¤±ã€åš Early Stoppingï¼ˆpatience=30ï¼‰
# 6. åœ¨ test set ä¸Šè¨ˆç®—ï¼š
#    Accuracyã€Precisionã€Recallã€F1-scoreã€æ··æ·†çŸ©é™£
# 7. main è£¡ä¸€æ¬¡è·‘ä¸‰å€‹æ¨¡å‹ï¼Œè¼¸å‡ºæ¯”è¼ƒè¡¨ + æ¯”è¼ƒåœ–
# 8. ä¾ç…§ç•¶ä¸‹æ™‚é–“èˆ‡è¶…åƒæ•¸å»ºç«‹å¯¦é©—è³‡æ–™å¤¾ï¼Œå°‡åœ–èˆ‡æ¬Šé‡åˆ†é¡å­˜æ”¾

import os
from datetime import datetime
import time

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models

from train_split import create_dataloaders  # ä½ çš„ train_split.py è£¡çš„å‡½å¼

import matplotlib.pyplot as plt
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    ConfusionMatrixDisplay,
)


# =======================
# 1. è‡ªè¨‚ 4-Block CNN æ¨¡å‹
# =======================
class SimpleCNN4Block(nn.Module):
    def __init__(self, num_classes=4):
        super(SimpleCNN4Block, self).__init__()

        self.features = nn.Sequential(
            # Block 1: 3 -> 16 -> 16, 256x256 -> 128x128
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
    

            nn.Conv2d(16, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),

            nn.MaxPool2d(2, 2),

            # Block 2: 16 -> 32 -> 32, 128x128 -> 64x64
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),

            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),

            nn.MaxPool2d(2, 2),

              # Block 3: 32 -> 64, 64 -> 32
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),

            # Block 4: 64 -> 128, 32 -> 16
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.MaxPool2d(2, 2),
        )

        # æœ€å¾Œ feature mapï¼š [32, 64, 64]
        self.classifier = nn.Sequential(
            nn.Linear(128 * 16 * 16, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes),
        )

    def forward(self, x):
        x = self.features(x)            # [B, 32, 64, 64]
        x = x.view(x.size(0), -1)       # [B, 32*64*64]
        x = self.classifier(x)
        return x

class SimpleCNN4Block_nomal(nn.Module):
    def __init__(self, num_classes=4):
        super(SimpleCNN4Block_nomal, self).__init__()

        self.features = nn.Sequential(
            # Block 1: 3 -> 16, 256x256 -> 128x128
            nn.Conv2d(3, 16, kernel_size=5, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # Block 2: 16 -> 32, 128x128 -> 64x64
            nn.Conv2d(16, 32, kernel_size=5, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # Block 3: 32 -> 64, 64x64 -> 32x32
            nn.Conv2d(32, 64, kernel_size=5, padding=2),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            # Block 4: 64 -> 128, 32x32 -> 16x16
            nn.Conv2d(64, 128, kernel_size=5, padding=2),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )

        # 256 -> 128 -> 64 -> 32 -> 16ï¼Œæ‰€ä»¥æœ€å¾Œæ˜¯ [128, 16, 16]
        self.classifier = nn.Sequential(
            nn.Linear(128 * 16 * 16, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes),
        )

    def forward(self, x):
        x = self.features(x)          # [B, 128, 16, 16]
        x = x.view(x.size(0), -1)     # [B, 128*16*16]
        x = self.classifier(x)        # [B, num_classes]
        return x
# =======================
# 2. æ¨¡å‹å·¥å» ï¼šä¾åå­—å»ºç«‹æ¨¡å‹
# =======================
def get_model(model_name="cnn", num_classes=4):
    model_name = model_name.lower()

    if model_name == "cnn":
        model = SimpleCNN4Block(num_classes=num_classes)

    elif model_name == "cnn_nomal":
        model = SimpleCNN4Block_nomal(num_classes=num_classes)

    elif model_name == "resnet18":
        model = models.resnet18(pretrained=True)
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)

    elif model_name == "resnet34":
        model = models.resnet34(pretrained=True)
        in_features = model.fc.in_features
        model.fc = nn.Linear(in_features, num_classes)

    else:
        raise ValueError(f"æœªçŸ¥çš„ model_name: {model_name}ï¼Œè«‹ç”¨ 'cnn'ã€'resnet18' æˆ– 'resnet34'")

    return model


# =======================
# 3. è¨“ç·´ / é©—è­‰ä¸€å€‹ epoch
# =======================
def train_one_epoch(model, loader, criterion, optimizer, device):
    # å°‡æ¨¡å‹åˆ‡æ›åˆ°ã€Œè¨“ç·´æ¨¡å¼ã€
    # æœƒå•Ÿå‹• Dropoutã€BatchNorm çš„ running mean/var æ›´æ–°
    model.train()

    running_loss = 0.0   # ç”¨ä¾†ç´¯ç©æ•´å€‹ epoch çš„ loss
    correct = 0          # æ­£ç¢ºé æ¸¬çš„æ•¸é‡
    total = 0            # æ¨£æœ¬æ•¸ç¸½è¨ˆ

    # å¾ DataLoader é€æ‰¹è®€å–è³‡æ–™
    for images, labels in loader:
        # å°‡å½±åƒèˆ‡æ¨™ç±¤ç§»åˆ° GPU æˆ– CPU
        images = images.to(device)
        labels = labels.to(device)

        # æ¢¯åº¦æ¸…é›¶ï¼ˆé¿å…å‰ä¸€æ‰¹è³‡æ–™çš„æ¢¯åº¦ç´¯ç©ï¼‰
        optimizer.zero_grad()

        # å‰å‘å‚³éï¼Œå¾—åˆ°æ¨¡å‹è¼¸å‡º logits
        outputs = model(images)

        # è¨ˆç®— lossï¼ˆå¦‚ CrossEntropyLossï¼‰
        loss = criterion(outputs, labels)

        # åå‘å‚³éï¼Œè¨ˆç®—æ¯å€‹åƒæ•¸çš„æ¢¯åº¦
        loss.backward()

        # ä½¿ç”¨ optimizer æ›´æ–°æ¨¡å‹çš„åƒæ•¸
        optimizer.step()

        # ç´¯ç© lossï¼ˆloss.item() æ˜¯å–®ä¸€ batch å¹³å‡ lossï¼Œæ‰€ä»¥ä¹˜ batch sizeï¼‰
        running_loss += loss.item() * images.size(0)

        # å–æœ€å¤§ logit çš„ index ä½œç‚ºé æ¸¬é¡åˆ¥
        _, preds = torch.max(outputs, 1)

        # è¨ˆç®—é€™æ‰¹æœ‰å¤šå°‘é æ¸¬æ­£ç¢º
        correct += (preds == labels).sum().item()

        # ç´¯åŠ  batch sizeï¼ˆæ­¤æ¬¡æœ‰å¤šå°‘è³‡æ–™ï¼‰
        total += labels.size(0)

    # è¨ˆç®—æ•´å€‹ epoch çš„å¹³å‡ loss èˆ‡ accuracy
    epoch_loss = running_loss / total
    epoch_acc = correct / total
    return epoch_loss, epoch_acc


def eval_one_epoch(model, loader, criterion, device):
    # åˆ‡æ›åˆ°ã€Œé©—è­‰æ¨¡å¼ã€
    # æœƒé—œé–‰ Dropoutã€BatchNorm ä¸å†æ›´æ–° running stats
    model.eval()

    running_loss = 0.0
    correct = 0
    total = 0

    # é©—è­‰ä¸éœ€è¦è¨ˆç®—æ¢¯åº¦ â†’ æ¸›å°‘è¨˜æ†¶é«”èˆ‡åŠ é€Ÿ
    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)

            # å‰å‘å‚³éï¼ˆæ²’æœ‰ backwardï¼‰
            outputs = model(images)

            # è¨ˆç®— loss
            loss = criterion(outputs, labels)

            # ç´¯ç© lossï¼ˆä¹˜ batch size æ˜¯ç‚ºäº†è¨ˆæ•´å€‹ epoch çš„å¹³å‡ lossï¼‰
            running_loss += loss.item() * images.size(0)

            # è¨ˆç®—é æ¸¬é¡åˆ¥
            _, preds = torch.max(outputs, 1)

            # ç´¯è¨ˆæ­£ç¢ºæ•¸
            correct += (preds == labels).sum().item()

            # ç´¯åŠ ç¸½æ•¸
            total += labels.size(0)

    # è¨ˆç®—æ•´å€‹ epoch çš„å¹³å‡ loss èˆ‡ accuracy
    epoch_loss = running_loss / total
    epoch_acc = correct / total
    return epoch_loss, epoch_acc



# =======================
# 4. åœ¨ test set ä¸Šæ‹¿ y_true / y_pred
# =======================
def get_test_predictions(model, test_loader, device):
    model.eval()
    y_true = []
    y_pred = []

    # å…¼å®¹å…©ç¨®æƒ…æ³ï¼š
    # 1) test_loader.dataset æ˜¯ Subset(ImageFolder)
    # 2) test_loader.dataset ç›´æ¥æ˜¯ ImageFolder
    ds = test_loader.dataset
    if hasattr(ds, "dataset") and hasattr(ds.dataset, "class_to_idx"):
        base_dataset = ds.dataset          # Subset è£¡åŒ…çš„ ImageFolder
    else:
        base_dataset = ds                  # ç›´æ¥å°±æ˜¯ ImageFolder

    idx_to_class = {v: k for k, v in base_dataset.class_to_idx.items()}
    class_names = [idx_to_class[i] for i in range(len(idx_to_class))]

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            _, preds = torch.max(outputs, 1)

            y_true.extend(labels.cpu().tolist())
            y_pred.extend(preds.cpu().tolist())

    return y_true, y_pred, class_names



# =======================
# 5. ç•«è¨“ç·´éç¨‹çš„ loss / acc åœ–ï¼ˆåŠ ä¸Šæ—©åœç·šï¼‰
# =======================
def plot_history(history, model_name, out_dir,
                 best_epoch=None, stop_epoch=None):
    """
    history: dictï¼Œå« "train_loss", "val_loss", "train_acc", "val_acc"
    best_epoch: é©—è­‰ loss æœ€ä½³çš„ epochï¼ˆæ—©åœå°æ‡‰çš„æœ€ä½³é»ï¼‰
    stop_epoch: å¯¦éš›è¨“ç·´åœæ­¢çš„ epochï¼ˆè§¸ç™¼ early stopping çš„é‚£å›åˆï¼‰
    out_dir: åœ–ç‰‡è¼¸å‡ºè³‡æ–™å¤¾ï¼ˆæœƒç”± train_model å‚³é€²ä¾†ï¼‰
    """
    os.makedirs(out_dir, exist_ok=True)
    epochs = range(1, len(history["train_loss"]) + 1)

    # ----- Loss æ›²ç·š -----
    plt.figure()
    plt.plot(epochs, history["train_loss"], label="Train Loss")
    plt.plot(epochs, history["val_loss"], label="Val Loss")

    if best_epoch is not None:
        plt.axvline(best_epoch, color="red", linestyle="--",
                    label=f"Best Val Loss (e{best_epoch})")
    if stop_epoch is not None and stop_epoch != best_epoch:
        plt.axvline(stop_epoch, color="gray", linestyle=":",
                    label=f"Stop Epoch (e{stop_epoch})")

    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f"{model_name} - Loss")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, f"{model_name}_loss.png"))
    plt.close()

    # ----- Accuracy æ›²ç·š -----
    plt.figure()
    plt.plot(epochs, history["train_acc"], label="Train Acc")
    plt.plot(epochs, history["val_acc"], label="Val Acc")

    if best_epoch is not None:
        plt.axvline(best_epoch, color="red", linestyle="--",
                    label=f"Best Val Loss (e{best_epoch})")
    if stop_epoch is not None and stop_epoch != best_epoch:
        plt.axvline(stop_epoch, color="gray", linestyle=":",
                    label=f"Stop Epoch (e{stop_epoch})")

    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title(f"{model_name} - Accuracy")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, f"{model_name}_acc.png"))
    plt.close()


# =======================
# 6. ç•«æ··æ·†çŸ©é™£
# =======================
def plot_confusion_matrix(y_true, y_pred, class_names, model_name, out_dir):
    os.makedirs(out_dir, exist_ok=True)

    cm = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)

    plt.figure(figsize=(5, 5))
    disp.plot(cmap="Blues", values_format="d")
    plt.title(f"{model_name} - Confusion Matrix")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, f"{model_name}_cm.png"))
    plt.close()


# =======================
# 7. ä¸»è¨“ç·´æµç¨‹ï¼ˆå« Early Stoppingï¼‰
# =======================
def train_model(
    model_name="cnn",   # æ¨¡å‹ç¨®é¡ï¼š'cnn' / 'resnet18' / 'resnet34'
    num_epochs=100,     # æœ€å¤§è¨“ç·´è¼ªæ•¸
    batch_size=64,
    lr=3e-4,
    seed=42,
    patience=30,
    exp_root="runs",    # å¯¦é©—æ ¹ç›®éŒ„ï¼ˆå¤–å±¤è³‡æ–™å¤¾ï¼Œç”± main æ±ºå®šï¼‰
    

):
    start_time = time.time()
    """
    exp_root: ä¸€æ¬¡å¯¦é©—çš„æ ¹ç›®éŒ„ï¼Œä¾‹å¦‚:
      runs/20251117_223045_bs32_lr0.001_pat30
    é€™è£¡é¢æœƒå†å»º model_name å­è³‡æ–™å¤¾ã€‚
    """

    # å°æ¯å€‹æ¨¡å‹å»ºç«‹è‡ªå·±çš„è³‡æ–™å¤¾ï¼Œä¾‹å¦‚ï¼š
    # runs/æ™‚é–“_bsxx_lrxx_patxx/cnn/

    model_dir = os.path.join(exp_root, model_name)
    plots_dir = os.path.join(model_dir, "plots")
    os.makedirs(model_dir, exist_ok=True)

    # 1ï¸âƒ£ DataLoader
    train_loader, val_loader, test_loader = create_dataloaders(
        batch_size=batch_size,
        seed=seed,
    )

    # 2ï¸âƒ£ é‹ç®—è£ç½® + GPU è³‡è¨Š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("ä½¿ç”¨è£ç½®:", device)

    if device.type == "cuda":
        print("=== CUDA è¨­å‚™è³‡è¨Š ===")
        print("GPU åç¨±:", torch.cuda.get_device_name(0))
        print("CUDA ç‰ˆæœ¬:", torch.version.cuda)
        print("cuDNN ç‰ˆæœ¬:", torch.backends.cudnn.version())
        total_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        reserved_mem = torch.cuda.memory_reserved(0) / (1024 ** 3)
        allocated_mem = torch.cuda.memory_allocated(0) / (1024 ** 3)
        free_mem = total_mem - allocated_mem
        print(f"GPU ç¸½è¨˜æ†¶é«”: {total_mem:.2f} GB")
        print(f"å·²ä¿ç•™è¨˜æ†¶é«”: {reserved_mem:.2f} GB")
        print(f"å·²é…ç½®è¨˜æ†¶é«”: {allocated_mem:.2f} GB")
        print(f"å¯ç”¨è¨˜æ†¶é«”:   {free_mem:.2f} GB")
        print("===================")
    else:
        print("ç›®å‰ä½¿ç”¨ CPUï¼Œç„¡æ³•é¡¯ç¤º GPU è©³ç´°è³‡è¨Šã€‚")

    # 3ï¸âƒ£ æ¨¡å‹ / Loss / Optimizer
    model = get_model(model_name, num_classes=4).to(device)
    print(f"\nä½¿ç”¨æ¨¡å‹: {model_name}")

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
    #optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=1e-4)
    #optimizer = torch.optim.RAdam(model.parameters(), lr=3e-4, weight_decay=1e-4)
    history = {
        "train_loss": [],
        "val_loss": [],
        "train_acc": [],
        "val_acc": [],
    }

    best_val_loss = float("inf")
    best_state_dict = None
    best_epoch = None          # ç´€éŒ„ val_loss æœ€ä½³çš„ epoch
    no_improve_count = 0

    print(f"\nå•Ÿç”¨ Early Stoppingï¼šä»¥é©—è­‰æå¤±ç‚ºæº–ï¼Œpatience = {patience}\n")
    print("ç›®å‰ç’°å¢ƒ","BS=",batch_size,"IR=",lr,"\n")

    # 4ï¸âƒ£ è¨“ç·´è¿´åœˆ
    for epoch in range(num_epochs):
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device
        )
        val_loss, val_acc = eval_one_epoch(
            model, val_loader, criterion, device
        )

        history["train_loss"].append(train_loss)
        history["val_loss"].append(val_loss)
        history["train_acc"].append(train_acc)
        history["val_acc"].append(val_acc)

        print(
            f"Epoch [{epoch+1}/{num_epochs}] "
            f"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | "
            f"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}"
        )

        # Early Stopping åˆ¤æ–·
        if val_loss < best_val_loss:
            # å„ªåŒ–æˆåŠŸ â†’ æ›´æ–°æœ€ä½³æ¨¡å‹ç´€éŒ„
            best_val_loss = val_loss
            best_state_dict = model.state_dict()# å­˜æ¨¡å‹æ¬Šé‡
            best_epoch = epoch + 1      # epoch å¾ 0 é–‹å§‹ï¼Œæ‰€ä»¥ +1
            no_improve_count = 0 #é‡ç½®
            print(f"  âœ é©—è­‰æå¤±ä¸‹é™ï¼Œæ–° best_val_loss = {best_val_loss:.4f}ï¼ˆepoch {best_epoch}ï¼‰")
        else:
            # æ²’é€²æ­¥ â†’ æ¬¡æ•¸ç´¯åŠ 
            no_improve_count += 1
            print(f"  âœ é©—è­‰æå¤±ç„¡é€²æ­¥ï¼ˆé€£çºŒ {no_improve_count} æ¬¡ï¼‰")
            
            # è‹¥ç„¡æå‡æ¬¡æ•¸é” patience â†’ è§¸ç™¼ Early Stopping
            if no_improve_count >= patience:
                print(f"\nâš  è§¸ç™¼ Early Stoppingï¼šé©—è­‰æå¤±å·²é€£çºŒ {patience} å€‹ epoch ç„¡é€²æ­¥ï¼Œåœæ­¢è¨“ç·´ã€‚\n")
                break # çµæŸè¨“ç·´è¿´åœˆ

# è¨“ç·´çµæŸå¾Œçš„å¯¦éš› epoch æ•¸ï¼ˆå¯èƒ½å›  Early Stopping å°‘æ–¼ num_epochsï¼‰
    stop_epoch = len(history["train_loss"])

    # 5ï¸âƒ£ è¼‰å…¥æœ€ä½³æ¬Šé‡
    if best_state_dict is not None:
        model.load_state_dict(best_state_dict)
        print(f"å·²è¼‰å…¥é©—è­‰æå¤±æœ€ä½³çš„æ¨¡å‹æ¬Šé‡ï¼ˆval_loss = {best_val_loss:.4f}ï¼Œepoch {best_epoch}ï¼‰")

    # 6ï¸âƒ£ Test set è©•ä¼°
    y_true, y_pred, class_names = get_test_predictions(model, test_loader, device)

    acc = accuracy_score(y_true, y_pred)
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )
    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
        y_true, y_pred, average="weighted", zero_division=0
    )

    print(f"\n[Test æŒ‡æ¨™ - {model_name}]")
    print(f"Accuracy:              {acc:.4f}")
    print(f"Precision (macro):     {precision_macro:.4f}")
    print(f"Recall    (macro):     {recall_macro:.4f}")
    print(f"F1-score  (macro):     {f1_macro:.4f}")
    print(f"Precision (weighted):  {precision_weighted:.4f}")
    print(f"Recall    (weighted):  {recall_weighted:.4f}")
    print(f"F1-score  (weighted):  {f1_weighted:.4f}")

    # 7ï¸âƒ£ ç•«è¨“ç·´æ›²ç·šï¼ˆå¸¶æ—©åœç·šï¼‰
    plot_history(
        history,
        model_name,
        out_dir=plots_dir,
        best_epoch=best_epoch,
        stop_epoch=stop_epoch,
    )

    # 8ï¸âƒ£ ç•«æ··æ·†çŸ©é™£
    plot_confusion_matrix(
        y_true, y_pred, class_names,
        model_name,
        out_dir=plots_dir,
    )

    # 9ï¸âƒ£ å„²å­˜æœ€ä½³æ¨¡å‹æ¬Šé‡ï¼ˆæ”¾åœ¨ model_dir åº•ä¸‹ï¼‰
    save_name = os.path.join(model_dir, f"corn_leaf_{model_name}.pth")
    torch.save(model.state_dict(), save_name)
    print(f"æ¨¡å‹å·²å„²å­˜ç‚º: {save_name}")

    metrics = {
        "accuracy": acc,
        "precision_macro": precision_macro,
        "recall_macro": recall_macro,
        "f1_macro": f1_macro,
        "precision_weighted": precision_weighted,
        "recall_weighted": recall_weighted,
        "f1_weighted": f1_weighted,
    }
    end_time = time.time()
    total_sec = end_time - start_time
    total_min = total_sec / 60

    print(f"\nğŸ“Œ {model_name} è¨“ç·´æ™‚é–“ï¼š")
    print(f"   â†’ {total_sec:.2f} ç§’ ({total_min:.2f} åˆ†é˜)")

    return metrics, history


# =======================
# 8. ä¸‰æ¨¡å‹æŒ‡æ¨™æ¯”è¼ƒè¡¨ï¼ˆè¼¸å‡ºæˆä¸€å¼µåœ–ï¼‰
# =======================
def plot_model_comparison(all_results, out_dir="plots"):
    """
    all_results: dictï¼Œkey æ˜¯ model_nameï¼Œ
                 value æ˜¯ train_model å›å‚³çš„ metrics dict
    out_dir: å¯¦é©—æ ¹ç›®éŒ„ï¼ˆä¾‹å¦‚ runs/æ™‚é–“_bsxx_lrxx_patxxï¼‰
    æœƒç”¢ç”Ÿä¸€å¼µè¡¨æ ¼åœ–ï¼š{out_dir}/model_comparison_table.png
    """
    os.makedirs(out_dir, exist_ok=True)

    models = list(all_results.keys())
    metrics_names = ["accuracy", "precision_macro", "recall_macro", "f1_macro"]

    # æº–å‚™è¡¨æ ¼è³‡æ–™
    table_data = [["Metric"] + models]
    for metric in metrics_names:
        row = [metric]
        for m in models:
            row.append(f"{all_results[m][metric]:.4f}")
        table_data.append(row)

    fig, ax = plt.subplots(figsize=(6, 1 + 0.5 * len(table_data)))
    ax.axis("off")
    table = ax.table(
        cellText=table_data,
        loc="center",
        cellLoc="center",
    )
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.auto_set_column_width(col=list(range(len(models) + 1)))
    plt.title("Model Comparison on Test Set", pad=10)
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "model_comparison_table.png"))
    plt.close()


# =======================
# 9. mainï¼šä¸€æ¬¡è·‘ CNN / ResNet18 / ResNet34
# =======================
if __name__ == "__main__":
    models_to_run = ["cnn", "resnet18", "resnet34"]

    # å…±ç”¨çš„è¶…åƒæ•¸ï¼ˆæœƒåæ˜ åœ¨è³‡æ–™å¤¾åç¨±ï¼‰
    batch_size = 64
    lr = 3e-4
    patience = 30
    num_epochs = 100
    seed = 42

    # ä¾ç•¶ä¸‹æ™‚é–“èˆ‡è¶…åƒæ•¸å»ºç«‹å¯¦é©—æ ¹ç›®éŒ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_name = f"{timestamp}_bs{batch_size}_lr{lr}_pat{patience}"
    exp_root = os.path.join("runs", exp_name)
    os.makedirs(exp_root, exist_ok=True)

    print(f"\næœ¬æ¬¡å¯¦é©—è¼¸å‡ºæœƒæ”¾åœ¨ï¼š{exp_root}\n")

    all_results = {}

    for name in models_to_run:
        print("\n" + "=" * 60)
        print(f"é–‹å§‹è¨“ç·´æ¨¡å‹ï¼š{name}")
        print("=" * 60)

        metrics, history = train_model(
            model_name=name,
            num_epochs=num_epochs,
            batch_size=batch_size,
            lr=lr,
            seed=seed,
            patience=patience,
            exp_root=exp_root,
        )
        all_results[name] = metrics

    print("\n\n===== ä¸‰å€‹æ¨¡å‹åœ¨ Test Set çš„æŒ‡æ¨™æ¯”è¼ƒ =====")
    for name, m in all_results.items():
        print(f"\næ¨¡å‹ï¼š{name}")
        print(f"  Accuracy:            {m['accuracy']:.4f}")
        print(f"  F1-score (macro):    {m['f1_macro']:.4f}")
        print(f"  F1-score (weighted): {m['f1_weighted']:.4f}")

    # ç•«ä¸€å¼µã€Œä¸‰å€‹æ¨¡å‹æŒ‡æ¨™æ¯”è¼ƒè¡¨æ ¼ã€åœ–ï¼ˆå­˜åˆ° exp_rootï¼‰
    plot_model_comparison(all_results, out_dir=exp_root)
    print(f"\nå·²è¼¸å‡ºæ¯”è¼ƒè¡¨åœ–ï¼š{os.path.join(exp_root, 'model_comparison_table.png')}")
